# -*- coding: utf-8 -*-
"""Classificacao_Estilo_Artistico

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gx4ughJOztpoyoQ0CJlFAIafqRSblNRa

Grupo: Mateus Padilha, Breno de Souza e Ian Esteves

# **DESAFIO**

Vamos trabalhar alguns cenários onde utilizaremos RNAs MLP para resolver problemas que envolvem imagens.

# **Classificação de estilo artístico por histograma de cores**

## **O problema**

O objetivo é classificar a corrente artística de uma pintura (ex: Impressionismo, Surrealismo, Renascimento).

## **O Conjunto de Dados**

Utilizaremos o dataset **"Painter by Numbers"** do [Kaggle](https://www.kaggle.com/competitions/painter-by-numbers/data), que contém milhares de pinturas rotuladas por artista, estilo e gênero. As informações sobre a base estão no mesmo link.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Carrega o dataset
df = pd.read_csv("all_data_info.csv")

# Verificar colunas disponíveis
print("Colunas:", df.columns.tolist())
print("Valores nulos por coluna:\n", df.isnull().sum())

# Caminho do arquivo CSV
df = pd.read_csv("all_data_info.csv")

# Verificar classes e balanceamento
print(df['style'].value_counts())

# Verificando valores ausentes em cada coluna
print("\nValores ausentes por coluna:")
print(df.isnull().sum())

# Porcentagem de valores ausentes
print("\nPorcentagem de valores ausentes:")
print((df.isnull().mean() * 100).round(2))

"""Observações importantes

Encontramos colunas com valores nulos (date ~25%).

A coluna alvo style tem alguns nulos (~0.95%) — esses foram removidos antes de treinar.
"""

# Quantos estilos únicos existem?
print("\nNúmero de estilos únicos:", df['style'].nunique())

# Distribuição dos estilos
print("\nDistribuição dos estilos:")
print(df['style'].value_counts().head(15))  # mostra os 20 mais comuns

# Visualizar graficamente
plt.figure(figsize=(10,6))
df['style'].value_counts().head(15).plot(kind='bar')
plt.title("Distribuição dos 15 estilos mais frequentes")
plt.xlabel("Estilo")
plt.ylabel("Número de imagens")
plt.xticks(rotation=45)
plt.show()

# Selecionar colunas relevantes

# Variável alvo
target = 'style'

# Features (numéricas + categóricas codificadas)
features = ['pixelsx', 'pixelsy', 'size_bytes', 'genre', 'source', 'artist_group']

# Remover linhas com valores nulos nessas colunas
df = df[features + [target]].dropna()

"""Escolhemos as colunas para treinar o modelo e removemos linhas com valores nulos nessas colunas. Optamos por usar esses metadados (resolução pixelsx/pixelsy, size_bytes, e colunas categóricas genre, source, artist_group) — porque são fáceis de extrair e já estão no CSV."""

# ============================================
# Codifica variáveis categóricas
# ============================================
df_encoded = df.copy()
for col in ['genre', 'source', 'artist_group']:
    le = LabelEncoder()
    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))

"""LabelEncoder: converte as categorias textuais para inteiros, permitindo que a MLP consuma essas features."""

# ============================================
# Balanceamento (oversampling)
# ============================================
max_size = df_encoded[target].value_counts().max()
lst = []

for class_index, group in df_encoded.groupby(target):
    lst.append(group.sample(max_size, replace=True))

df_balanced = pd.concat(lst).reset_index(drop=True)

print("Distribuição após balanceamento:")
print(df_balanced[target].value_counts())

"""Aumenta todas as classes até o tamanho da maior, produzindo um dataset mais balanceado, para evitar que classes majoritárias dominem o aprendizado."""

# ============================================
# Preparar X e y
# ============================================
X = df_balanced[features].values
y = df_balanced[target].values

# Codifica o rótulo (style)
le_target = LabelEncoder()
y_encoded = le_target.fit_transform(y)
y_categorical = to_categorical(y_encoded)

# Normaliza os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""LabelEncoder + to_categorical: converte labels para vetores one-hot exigidos pelo categorical_crossentropy.

StandardScaler: centraliza e escala features numéricas, o que ajuda convergência da rede (camadas densas respondem melhor a features padronizadas).
"""

# ============================================
# Divisão em treino e teste
# ============================================
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_categorical, test_size=0.2, random_state=42
)

# ============================================
# Criação e treinanamento da MLP
# ============================================
model = Sequential([
    Dense(128, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(y_train.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=30,
    batch_size=128,
    verbose=1
)

"""Arquitetura: 2 camadas densas (128 -> 64) com relu, dropout 0.3 para regularização e camada final softmax para classificação multiclasses.

Loss categorical_crossentropy e otimizador adam: escolhas padrão eficazes.

batch_size=128 e epochs=30: valores razoáveis para começar.

Dropout ajuda a mitigar overfitting.
"""

# ============================================
# 9. Avaliar o modelo
# ============================================
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nAcurácia no teste: {acc:.4f}")

# ============================================
# Curva de aprendizado (Acurácia)
# ============================================
plt.figure(figsize=(12,5))

# Acurácia
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Treino')
plt.plot(history.history['val_accuracy'], label='Validação')
plt.title('Evolução da Acurácia')
plt.xlabel('Épocas')
plt.ylabel('Acurácia')
plt.legend()

plt.tight_layout()
plt.show()

# ============================================
# Ranking de desempenho por estilo
# ============================================

# Calcular métricas por classe
precisions = precision_score(y_true, y_pred_classes, average=None, zero_division=0)
recalls = recall_score(y_true, y_pred_classes, average=None, zero_division=0)
f1s = f1_score(y_true, y_pred_classes, average=None, zero_division=0)

df_metrics = pd.DataFrame({
    'Style': le_target.classes_,
    'Precision': precisions,
    'Recall': recalls,
    'F1-Score': f1s
}).sort_values(by='F1-Score', ascending=False)

# Top 10
top10 = df_metrics.head(10)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.barh(top10['Style'], top10['F1-Score'], color='green')
plt.title('Top 10 estilos classificados')
plt.gca().invert_yaxis()

plt.tight_layout()
plt.show()

"""Referências principais de documentação:

-> Slides de Redes Neurais, Gemini e ChatGPT
"""